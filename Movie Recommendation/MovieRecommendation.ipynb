{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HACETTEPE UNIVERSITY\n",
    "\n",
    "Machine Learning Laboratory Assignment1\n",
    "\n",
    "Name:FURKAN YILMAZ\n",
    "\n",
    "Number:21527621\n",
    "\n",
    "Subject:Movie Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTRODUCTION\n",
    "\n",
    "In the first part of this assignment, we are asked to solve the theoretical questions given to understand kNN and kernel regression algorithms.\n",
    "\n",
    "In the second part, we are asked to recommend a movie to the users by using the kNN and weighted kNN algorithm, and to predict using the rating of the neighbors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Part I: Theory Questions\n",
    "\n",
    "### k-Nearest Neighbor Classification\n",
    "\n",
    "1. The K nn algorithm is easy to implement but works quite slowly on large data sets. In large data sets, the number of variables is too high and the number of variables is too high causing the Knn algorithm to run slower.\n",
    "\n",
    "2. a b a b a b a b  ------->     is a 1-Dimensional dataset.\n",
    "\n",
    "When we select any point in this dataset for testing, we always take the error value as 1 because the closest point is not from its own class.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. \t1) Since the Euclidean distance is used in the Knn algorithm, the test instance is assigned to (-) class since k = 1 is the closest point to that test (-).\n",
    "\t\n",
    "2) When the euclidean distance is used for k = 3, the test sample is assigned to (-) since 2 of the nearest 3 points are (-) and 1 is (+).\n",
    "\t\n",
    "3) When euclidean distance is used for k = 5, the test sample is assigned to (+) class since 3 of the nearest 5 points are (+) and 2 of them are (-).\n",
    "\n",
    "4. \t\n",
    "\n",
    "1) T\n",
    "\n",
    "2) T\n",
    "\n",
    "3) T\n",
    "\n",
    "###  Linear Regression\n",
    "\n",
    "1. The mean of x2 is 5.489,4 and the range is 8464 - 2025 = 6439.\n",
    "x2^4 =  (4489 - 5489) / 6439 = - 0,155.\n",
    "\n",
    "2. We always consider residual as vertical offsets. Perpendicular offset are useful in case of PCA.\n",
    "\n",
    "3.<img src = \"20191103_165735.jpg\" >\n",
    "    \n",
    "4. Some data sets may show different characteristics in terms of size, variety  of variables, and range. In these cases, some features may be irrelevant or misleading. Feature scaling becomes meaningful in these cases. For example, if one  data in  the data set is larger, algorithms using euclidean distance need to normalize that data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCLUSION\n",
    "\n",
    "The error values I received when I applied kNN and weigtedKNN came very close to each other. I think it's because I didn't normalize the data. I found these values close to each other because I didn't normalize the data. I use different k numbers and got different results. Choosing k too small or too large gives incorrect results. While I set k to 7, 9 or 11, I usually find fewer errors. My error values are usually between 0.9 and 0.7, which may be due to the fact that the data is not cleared of bad data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAMPLE OUTPUT\n",
    "\n",
    " \n",
    " k=11 and in this case the best model is 6th.\n",
    " \n",
    "     1. Model Mean Absolute Error:0.8103398534607292\n",
    "     1. Model weighted Error: 0.8094063876896627\n",
    "\n",
    "     2. Model Mean Absolute Error:0.7739177080268705\n",
    "     2. Model weighted Error: 0.7752481005699442\n",
    "\n",
    "     3. Model Mean Absolute Error:0.7947319985162703\n",
    "     3. Model weighted Error: 0.7949591410305946\n",
    "\n",
    "     4. Model Mean Absolute Error:0.7894283405645401\n",
    "     4. Model weighted Error: 0.7910210770538475\n",
    "\n",
    "     5. Model Mean Absolute Error:0.8271934461187943\n",
    "     5. Model weighted Error: 0.8294641036465963\n",
    "\n",
    "     6. Model Mean Absolute Error:0.7404685713445363\n",
    "     6. Model weighted Error: 0.7406271894095711\n",
    "\n",
    "     7. Model Mean Absolute Error:0.8212376482597372\n",
    "     7. Model weighted Error: 0.824360582139224\n",
    "\n",
    "     8. Model Mean Absolute Error:0.8706121020418575\n",
    "     8. Model weighted Error: 0.8706160012100945\n",
    "\n",
    "     9. Model Mean Absolute Error:0.8122244949560911\n",
    "     9. Model weighted Error: 0.811873015825695\n",
    "\n",
    "     10. Model Mean Absolute Error:0.764928386462959\n",
    "     10. Model weighted Error: 0.7667835308248643\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have read the supplied input files with the read_csv function of the pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "readLinks=pd.read_csv(\"links.csv\")\n",
    "readMovies=pd.read_csv(\"movies.csv\")\n",
    "readRatings=pd.read_csv(\"ratings_train.csv\")\n",
    "readTags=pd.read_csv(\"tags.csv\")\n",
    "valid = pd.read_csv('ratings_test_without_gt_new.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I combined the merge function with ratings dataframe and the movies dataframe. I created another dataframe that deleted the title genres and timestamp columns, and then sorted this dataframe by movieId column for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= readMovies.merge(readRatings,how='inner')\n",
    "df2=df1.sort_values(by=['userId'])\n",
    "\n",
    "\n",
    "df4=df2.drop(df1.columns[[1,2,5]],axis=1)\n",
    "df4=df4.sort_values(by=['movieId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to calculate the cosine similarity of the two input vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineBasedSimilarty(x,y):\n",
    "    dot = np.dot(x, y)\n",
    "    norm_x = np.linalg.norm(x)\n",
    "    norm_y = np.linalg.norm(y)\n",
    "    cosineSim = dot / (norm_x * norm_y)\n",
    "    return cosineSim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Knn function calculates the cosine similarity of each user in the test separately with each user in the train file.It uses the cosineBasedSimilarity function when calculating this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNN(testArray,trainArray):\n",
    "    similarityArray=[]\n",
    "    for i in range(len(testArray)):\n",
    "        newArray=[]\n",
    "        for j in range(len(trainArray)):\n",
    "            x= cosineBasedSimilarty(testArray[i],trainArray[j])\n",
    "            newArray.append(x)\n",
    "        similarityArray.append(newArray)\n",
    "    userBySimilarty=np.array(similarityArray)\n",
    "    return userBySimilarty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neighbor function uses the table with cosine similarities to find the number of neighbors given the number of k and adds them to a dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def neighbor(k,newTable):\n",
    "    dictt={}\n",
    "    neighborsCosineDict={}\n",
    "    for i in range(len(newTable)):\n",
    "        similarityArray=[]\n",
    "        neighborArray = []\n",
    "        keys = newTable.index[i]\n",
    "        for j in range(k):\n",
    "            x = newTable.loc[newTable.index[i]].sort_values(ascending=False)\n",
    "            similarityArray.append(x.values[j])\n",
    "            neighborArray.append(x.index[j])\n",
    "        neighborsCosineDict[keys]=similarityArray\n",
    "        dictt[keys]=neighborArray\n",
    "    return dictt,neighborsCosineDict\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meanAbsoluteError function uses the dataframe that holds the ratings given by its neighbors to the movies each test user  watches.Using this dataframe, predicts the rating that the user should give to the film, taking the average of neighbors.It then finds the difference between the test user's vote and the estimated value, and does this for all movies and calculates the user's error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanAbsoluteError(table,neigs2):\n",
    "    columndict2={}\n",
    "    testdict2={}\n",
    "    for i in range(len(table.columns)):\n",
    "        a=table.columns[i]\n",
    "        array=table[a].values\n",
    "        counter=0\n",
    "        avg=0\n",
    "        for j in range(len(array)):\n",
    "            if(array[j]!=0):\n",
    "                counter=counter+1\n",
    "                avg=avg+array[j]\n",
    "        if(counter!=0):\n",
    "            avg = avg/counter\n",
    "            columndict2[a]=avg\n",
    "            testdict2[a]= neigs2[i]\n",
    "    errorByMovie=0\n",
    "    for j in range(len(testdict2)): \n",
    "        errorByMovie += abs(list(testdict2.values())[j] - list(columndict2.values())[j])\n",
    "    errorByMovie = errorByMovie/ len(testdict2)\n",
    "    return errorByMovie\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weightedError function is similar to the meanAbsoluteError function. The only difference is multiplied by the cosine similarities of the ratings given and divided by the sum of the cosine similarities of the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightedError(simArray,table,neigs2):\n",
    "    columndict={}\n",
    "    testdict={}\n",
    "    for i in range(len(table.columns)):\n",
    "        a=table.columns[i]\n",
    "        array=table[a].values\n",
    "        sumOfArray=0\n",
    "        avg=0\n",
    "        for j in range(len(array)):\n",
    "            if(array[j]!=0):\n",
    "                sumOfArray += simArray[j]\n",
    "                avg=avg+(array[j] * simArray[j])\n",
    "        if(sumOfArray!=0):\n",
    "            avg = avg/sumOfArray\n",
    "            columndict[a]=avg\n",
    "            testdict[a]= neigs2[i]\n",
    "\n",
    "    errorByMovie2 = 0\n",
    "    for j in range(len(testdict)): \n",
    "        errorByMovie2 += abs(list(testdict.values())[j] - list(columndict.values())[j])\n",
    "    errorByMovie2 = errorByMovie2 / len(testdict)\n",
    "    return errorByMovie2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculateError function creates variables to send to the meanAbsoluteError and weigtedError functions and passes these functions as parameters. Calculates the overall error rate after finding the error values of each test user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculateError(testArray,trainArray,dictt,neighborsCosineDict,i):  \n",
    "    TotalError=0\n",
    "    weigtedKnnError=0\n",
    "    for j in range(len(testArray)):\n",
    "        df3=pd.DataFrame.from_dict(dictt,orient='index')\n",
    "        b=list(dictt.keys())[j]\n",
    "        a=df3.iloc[j].values\n",
    "\n",
    "        real_index =[dataTable_indices[i] for i in a]\n",
    "        indices = dataTable2.index == np.repeat(b, len(dataTable2))\n",
    "\n",
    "        aa = dataTable[indices]\n",
    "        bb = aa.values !=0\n",
    "        control = df4['userId'] == list(dictt.keys())[j]\n",
    "        c=df4[control]['movieId'].values\n",
    "        \n",
    "        neigs = dataTable.values[real_index]\n",
    "        neigs = neigs[:, bb.tolist()[0]]\n",
    "        table=pd.DataFrame(neigs,index=a,columns=c)\n",
    "        test_index=dataTable_indices[b]\n",
    "        test_index_rates=dataTable[indices]\n",
    "        testMoviesRates = test_index_rates.values != 0\n",
    "\n",
    "        neigs2 = dataTable.values[test_index]\n",
    "        neigs2 = neigs2[testMoviesRates.tolist()[0]]\n",
    "\n",
    "        tmp = meanAbsoluteError(table,neigs2)\n",
    "        TotalError = TotalError + tmp\n",
    "\n",
    "        simArray=list(neighborsCosineDict.values())[j]\n",
    "        \n",
    "        tmp2 = weightedError(simArray,table,neigs2)\n",
    "        \n",
    "        weigtedKnnError = weigtedKnnError + tmp2\n",
    "        \n",
    "    TotalError = TotalError/ (len(testArray))\n",
    "    weigtedKnnError = weigtedKnnError / (len(testArray))\n",
    "    print(\" {}. Model Mean Absolute Error:{}\".format(i,TotalError))    \n",
    "    print(\" {}. Model weighted Error: {}\".format(i,weigtedKnnError))\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section have been created pivot tables to be used.The values of the dataTable variable NaN are replaced with 0.This is done to calculate cosine similarity. The original file is shuffeled to determine the train file and the test file. A dictionary is created which specifies which index holds which userId for future use. I created a new dataFrame with the return value from knn function. the rows of this Dataframe are the users in the test, the columns are the trains, and the values of the cosine similarities.I did cross validation and called the calculateError function for each model and tried to see the most appropriate model by pressing the general error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. Model Mean Absolute Error:0.7944159121939757\n",
      " 1. Model weighted Error: 0.7972420371508788\n",
      "\n",
      " 2. Model Mean Absolute Error:0.8031554324074844\n",
      " 2. Model weighted Error: 0.8068170841677031\n",
      "\n",
      " 3. Model Mean Absolute Error:0.7828916062898694\n",
      " 3. Model weighted Error: 0.7832461930449502\n",
      "\n",
      " 4. Model Mean Absolute Error:0.8231354739625668\n",
      " 4. Model weighted Error: 0.8242404553275273\n",
      "\n",
      " 5. Model Mean Absolute Error:0.792030181878234\n",
      " 5. Model weighted Error: 0.7936509804061189\n",
      "\n",
      " 6. Model Mean Absolute Error:0.773159008669674\n",
      " 6. Model weighted Error: 0.7733065989384583\n",
      "\n",
      " 7. Model Mean Absolute Error:0.801895155484714\n",
      " 7. Model weighted Error: 0.8019403016918912\n",
      "\n",
      " 8. Model Mean Absolute Error:0.807572358892595\n",
      " 8. Model weighted Error: 0.8081944827902608\n",
      "\n",
      " 9. Model Mean Absolute Error:0.8690682306211903\n",
      " 9. Model weighted Error: 0.8704877915514165\n",
      "\n",
      " 10. Model Mean Absolute Error:0.8338952607303363\n",
      " 10. Model weighted Error: 0.8355444587656525\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataTable = pd.pivot_table(df1,values=\"rating\",index='userId',columns='movieId')\n",
    "dataTable2 = pd.pivot_table(df1,values=\"rating\",index='userId',columns='movieId')\n",
    "dataTable.fillna(value=0, inplace=True)\n",
    "dataTable_indices = {}\n",
    "\n",
    "for counter, i in enumerate(dataTable2.index):\n",
    "    dataTable_indices[i] = counter\n",
    "\n",
    "shuffleTable2=dataTable.iloc[np.random.permutation(len(dataTable))]\n",
    "\n",
    "index=int((len(shuffleTable2))/10)\n",
    "count=0\n",
    "\n",
    "for i in range(10):\n",
    "    if(i!=0):\n",
    "        if(i==10):\n",
    "            trainTable=shuffleTable2.iloc[0:count , :]\n",
    "            testTable = shuffleTable2.iloc[count : , : ]\n",
    "        else:\n",
    "            train=shuffleTable2.iloc[0:count,:]\n",
    "            \n",
    "            testTable, trainTable = (shuffleTable2.iloc[count : index, : ], shuffleTable2.iloc[index : , : ])\n",
    "            trainTable=pd.concat([train,trainTable])            \n",
    "    else:      \n",
    "        testTable, trainTable = (shuffleTable2.iloc[count : index, : ], shuffleTable2.iloc[index : , : ])\n",
    "\n",
    "    index+=int((len(shuffleTable2))/10)\n",
    "    count+=int((len(shuffleTable2))/10)\n",
    "    testArray=np.array(testTable)\n",
    "    trainArray = np.array(trainTable)\n",
    "    testIndex=testTable.index\n",
    "\n",
    "    similarity = kNN(testArray,trainArray)\n",
    "\n",
    "    newTable = pd.DataFrame(similarity,index=testTable.index)\n",
    "    newTable.columns=trainTable.index\n",
    "    \n",
    "    a,b = neighbor(11,newTable)\n",
    "\n",
    "    \n",
    "    calculateError(testArray,trainArray,a,b,i+1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
